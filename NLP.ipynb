{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "713c8b5d-94a5-4263-a536-48e8742d026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.11.12-cp310-cp310-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (24.2)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.5.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.1.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.0-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.18.3-cp310-cp310-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached aiohttp-3.11.12-cp310-cp310-win_amd64.whl (442 kB)\n",
      "Downloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow-19.0.1-cp310-cp310-win_amd64.whl (25.3 MB)\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 8.7/25.3 MB 41.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 14.9/25.3 MB 34.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.7/25.3 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.3 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.3/25.3 MB 28.6 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl (102 kB)\n",
      "Using cached frozenlist-1.5.0-cp310-cp310-win_amd64.whl (51 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.3.0-cp310-cp310-win_amd64.whl (44 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached yarl-1.18.3-cp310-cp310-win_amd64.whl (90 kB)\n",
      "Installing collected packages: xxhash, urllib3, tqdm, pyyaml, pyarrow, propcache, multidict, idna, frozenlist, dill, charset-normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, yarl, requests, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.1.0 certifi-2025.1.31 charset-normalizer-3.4.1 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 huggingface-hub-0.29.1 idna-3.10 multidict-6.1.0 multiprocess-0.70.16 propcache-0.3.0 pyarrow-19.0.1 pyyaml-6.0.2 requests-2.32.3 tqdm-4.67.1 urllib3-2.3.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f054b4-b884-496d-a942-39d8ae05ea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (0.21.0+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (2.6.0+cu118)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: datasets in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (3.3.2)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: torch==2.6.0+cu118 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torchvision) (2.6.0+cu118)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torch==2.6.0+cu118->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torch==2.6.0+cu118->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torch==2.6.0+cu118->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torch==2.6.0+cu118->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torch==2.6.0+cu118->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torch==2.6.0+cu118->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from sympy==1.13.1->torch==2.6.0+cu118->torchvision) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: psutil in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from jinja2->torch==2.6.0+cu118->torchvision) (2.1.5)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.6/10.0 MB 16.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.0 MB 25.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 19.4 MB/s eta 0:00:00\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: safetensors, regex, tokenizers, accelerate, transformers, evaluate\n",
      "Successfully installed accelerate-1.4.0 evaluate-0.4.3 regex-2024.11.6 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£å¥—ä»¶\n",
    "!pip install torchvision torchaudio transformers datasets evaluate accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fbabde8-1da5-4118-8264-ce3121e31dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "<torch.cuda.device object at 0x00000223B819B100>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edf31ff7-2e82-49c2-a824-04f8a534607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "import random\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9e0cdaa-5617-4107-997b-642d71fca34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Johnson8187/Chinese_Multi-Emotion_Dialogue_Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c00c3e14-293f-4eae-8bbc-9744c22e9077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       text emotion\n",
      "0                 ä½ è¦ä¸è¦å»åƒåˆé¤ï¼Ÿ    å¹³æ·¡èªæ°£\n",
      "1                èª’èª’èª’ï¼æˆ‘ç”„é¸ä¸Šäº†ï¼    é–‹å¿ƒèªèª¿\n",
      "2        æˆ‘å¹¾å¤©èº«é«”å¥½åƒæœ‰é»ä¸å¤ªèˆ’æœï¼Œè‚šå­å¥½ç—›    æ‚²å‚·èªèª¿\n",
      "3     æˆ‘çš„å°å°ˆé¡Œçµ„å“¡éƒ½ä¸åšäº‹ï¼Œå¹¹!è¶…å¾Œæ‚”è·Ÿä»–ä¸€çµ„    æ†¤æ€’èªèª¿\n",
      "4          ä»–å€‘æ˜¯ä¸æ˜¯åµæ¶äº†ï¼Ÿä¸æœƒæ‰“èµ·ä¾†å§ï¼Ÿ    å¹³æ·¡èªæ°£\n",
      "...                     ...     ...\n",
      "4154   ç‚ºä»€éº¼æ¯æ¬¡ç¸½æ˜¯é€™éº¼è¤‡é›œï¼Œæˆ‘åˆ°åº•è©²æ€éº¼è¾¦ï¼Ÿ    ç–‘å•èªèª¿\n",
      "4155      é€™ä¸æ‡‰è©²æ˜¯é€™æ¨£ï¼Œç‚ºä»€éº¼æˆ‘ç¸½æ˜¯å¼„éŒ¯ï¼Ÿ    ç–‘å•èªèª¿\n",
      "4156       æˆ‘æ²’æœ‰è½éŒ¯å§ï¼Ÿé€™çœŸçš„æ˜¯ç™¼ç”Ÿäº†å—ï¼Ÿ    ç–‘å•èªèª¿\n",
      "4157       ä½ åˆ°åº•æ€éº¼æƒ³çš„ï¼Ÿé€™æ˜¯ä½ çš„è¨ˆåŠƒå—ï¼Ÿ    ç–‘å•èªèª¿\n",
      "4158    é€™çœŸçš„æ˜¯æˆ‘æƒ³è¦çš„çµæœå—ï¼ŸçœŸæ˜¯å¤ªé©šè¨äº†ï¼    é©šå¥‡èªèª¿\n",
      "\n",
      "[4159 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"hf://datasets/Johnson8187/Chinese_Multi-Emotion_Dialogue_Dataset/data.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01d9fe94-e483-4aab-9448-26e8b3a4b580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¹³æ·¡èªæ°£\n"
     ]
    }
   ],
   "source": [
    "print(df['emotion'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18595c0f-192b-4465-9cd3-30e42fc7d0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\anaconda3\\envs\\dl\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\student\\.cache\\huggingface\\hub\\models--google-bert--bert-base-chinese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3327/3327 [00:00<00:00, 4293.66 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 832/832 [00:00<00:00, 4383.83 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\student\\anaconda3\\envs\\dl\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [156/156 2:47:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.478698</td>\n",
       "      <td>0.853399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('output\\\\tokenizer_config.json',\n",
       " 'output\\\\special_tokens_map.json',\n",
       " 'output\\\\vocab.txt',\n",
       " 'output\\\\added_tokens.json',\n",
       " 'output\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "'''\n",
    "è¨­å®š hyperparameters\n",
    "'''\n",
    "model_name = 'google-bert/bert-base-chinese'\n",
    "max_seq_length = 512\n",
    "num_labels = 8  # ä¿®æ”¹ç‚º8ï¼Œå› ç‚ºæ‚¨æœ‰8ç¨®æƒ…ç·’æ¨™ç±¤\n",
    "output_dir = 'output'\n",
    "\n",
    "# è®€å–è¨“ç·´è³‡æ–™\n",
    "df = pd.read_csv(\"hf://datasets/Johnson8187/Chinese_Multi-Emotion_Dialogue_Dataset/data.csv\")\n",
    "sentences = df['text']\n",
    "emotion_map = {\n",
    "    \"å¹³æ·¡èªæ°£\": 0,\n",
    "    \"é—œåˆ‡èªèª¿\": 1,\n",
    "    \"é–‹å¿ƒèªèª¿\": 2,\n",
    "    \"æ†¤æ€’èªèª¿\": 3,\n",
    "    \"æ‚²å‚·èªèª¿\": 4,\n",
    "    \"ç–‘å•èªèª¿\": 5,\n",
    "    \"é©šå¥‡èªèª¿\": 6,\n",
    "    \"å­æƒ¡èªèª¿\": 7\n",
    "}\n",
    "\n",
    "# å°‡æ–‡å­—æ¨™ç±¤è½‰æ›ç‚ºæ•¸å­—\n",
    "labels = df['emotion'].map(emotion_map)\n",
    "\n",
    "# è¼‰å…¥ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# å»ºç«‹å®Œæ•´çš„ Datasetï¼ˆåŒ…å«æ–‡æœ¬å’Œæ¨™ç±¤ï¼‰\n",
    "dataset = Dataset.from_dict({\n",
    "    'text': sentences,\n",
    "    'labels': labels\n",
    "})\n",
    "\n",
    "# åˆ‡åˆ†è¨“ç·´é›†å’Œæ¸¬è©¦é›†\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# å®šç¾©é è™•ç†å‡½æ•¸\n",
    "def preprocess_function(examples):\n",
    "    # ä½¿ç”¨ tokenizer è™•ç†æ–‡æœ¬\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=None  # é‡è¦ï¼šä¸è¦åœ¨é€™è£¡æŒ‡å®š return_tensors\n",
    "    )\n",
    "    # ç¢ºä¿åŒ…å«æ¨™ç±¤\n",
    "    tokenized['labels'] = examples['labels']\n",
    "    return tokenized\n",
    "\n",
    "# å°æ•¸æ“šé›†é€²è¡Œé è™•ç†\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names  # ç§»é™¤åŸå§‹æ¬„ä½\n",
    ")\n",
    "\n",
    "# è®€å–æ¨¡å‹\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# è¨­å®šè¨“ç·´åƒæ•¸\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=0.00005,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",  # ä¿®æ­£åƒæ•¸åç¨±\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# è¨ˆç®—æ¨¡å‹è©•ä¼°æŒ‡æ¨™\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')  # ä½¿ç”¨weightedå› ç‚ºæ˜¯å¤šåˆ†é¡å•é¡Œ\n",
    "    return {\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "# è¨­å®š Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# é–‹å§‹è¨“ç·´\n",
    "trainer.train()\n",
    "\n",
    "# å„²å­˜æ¨¡å‹\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "# å„²å­˜ tokenizer\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d090f53f-d58d-40a4-bba2-7da3389ace2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_2', 'score': 0.8571394681930542},\n",
      " {'label': 'LABEL_4', 'score': 0.9074247479438782},\n",
      " {'label': 'LABEL_4', 'score': 0.9293628931045532},\n",
      " {'label': 'LABEL_2', 'score': 0.6840886473655701},\n",
      " {'label': 'LABEL_0', 'score': 0.9081459045410156}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline,\n",
    ")\n",
    "from pprint import pprint\n",
    "\n",
    "model_dir = './output'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "pipe = pipeline(task='text-classification', model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "texts = [\n",
    "\t\"æˆ‘æ¯å¤©éƒ½èƒ½è·Ÿå¥¹ä¸€èµ·ä¸Šå­¸ï¼Œæˆ‘å¥½é–‹å¿ƒï¼\",\n",
    "\t\"æœ€å¥½çš„æœ‹å‹è¦é›¢é–‹è‡ºç£äº†ï¼Œä»¥å¾Œå¯èƒ½ä¸å®¹æ˜“å†è¦‹é¢...\",\n",
    "\t\"æˆ‘è¦ºå¾—æˆ‘å¿«ä¸è¡Œäº†...\",\n",
    "\t\"å‰›å‰›æ”¶åˆ°ç ”ç©¶æ‰€éŒ„å–çš„é€šçŸ¥æ›¸ï¼\",\n",
    "\t\"ä»Šå¹´çš„å†¬å¤©å¥½åƒæ¯”è¼ƒæ™šä¾†ã€‚\"\n",
    "]\n",
    "result = pipe(texts)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc85f29-479a-4db0-90f8-5d2b3fed2d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from pprint import pprint\n",
    "\n",
    "model_dir = './output'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "pipe = pipeline(task='text-classification', model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# å»ºç«‹æ¨™ç±¤åˆ°æƒ…ç·’çš„åå‘æ˜ å°„\n",
    "label_to_emotion = {str(v): k for k, v in emotion_map.items()}\n",
    "\n",
    "def predict_with_emotion(texts):\n",
    "    results = pipe(texts)\n",
    "    formatted_results = []\n",
    "\n",
    "    for text, result in zip(texts, results):\n",
    "        # å¾ 'LABEL_X' æå–æ•¸å­—\n",
    "        label_num = result['label'].split('_')[1]\n",
    "        emotion = label_to_emotion[label_num]\n",
    "        formatted_results.append({\n",
    "            'text': text,\n",
    "            'emotion': emotion,\n",
    "            'confidence': round(result['score'], 3)\n",
    "        })\n",
    "\n",
    "    return formatted_results\n",
    "\n",
    "# æ¸¬è©¦æ–‡æœ¬\n",
    "texts = [\n",
    "\t\"æˆ‘æ¯å¤©éƒ½èƒ½è·Ÿå¥¹ä¸€èµ·ä¸Šå­¸ï¼Œæˆ‘å¥½é–‹å¿ƒï¼\",\n",
    "\t\"æœ€å¥½çš„æœ‹å‹è¦é›¢é–‹è‡ºç£äº†ï¼Œä»¥å¾Œå¯èƒ½ä¸å®¹æ˜“å†è¦‹é¢...\",\n",
    "\t\"æˆ‘è¦ºå¾—æˆ‘å¿«ä¸è¡Œäº†...\",\n",
    "\t\"å‰›å‰›æ”¶åˆ°ç ”ç©¶æ‰€éŒ„å–çš„é€šçŸ¥æ›¸ï¼\",\n",
    "\t\"ä»Šå¹´çš„å†¬å¤©å¥½åƒæ¯”è¼ƒæ™šä¾†ã€‚\"\n",
    "]\n",
    "\n",
    "results = predict_with_emotion(texts)\n",
    "for result in results:\n",
    "    print(f\"{result['text']} => {result['emotion']} ({result['confidence']})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
