{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "713c8b5d-94a5-4263-a536-48e8742d026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.11.12-cp310-cp310-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (24.2)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.5.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.1.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.0-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.18.3-cp310-cp310-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached aiohttp-3.11.12-cp310-cp310-win_amd64.whl (442 kB)\n",
      "Downloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow-19.0.1-cp310-cp310-win_amd64.whl (25.3 MB)\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 8.7/25.3 MB 41.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 14.9/25.3 MB 34.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.7/25.3 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.3 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.3/25.3 MB 28.6 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl (102 kB)\n",
      "Using cached frozenlist-1.5.0-cp310-cp310-win_amd64.whl (51 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.3.0-cp310-cp310-win_amd64.whl (44 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached yarl-1.18.3-cp310-cp310-win_amd64.whl (90 kB)\n",
      "Installing collected packages: xxhash, urllib3, tqdm, pyyaml, pyarrow, propcache, multidict, idna, frozenlist, dill, charset-normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, yarl, requests, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.1.0 certifi-2025.1.31 charset-normalizer-3.4.1 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 huggingface-hub-0.29.1 idna-3.10 multidict-6.1.0 multiprocess-0.70.16 propcache-0.3.0 pyarrow-19.0.1 pyyaml-6.0.2 requests-2.32.3 tqdm-4.67.1 urllib3-2.3.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f054b4-b884-496d-a942-39d8ae05ea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (0.21.0+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (2.6.0+cu118)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: datasets in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (3.3.2)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: torch==2.6.0+cu118 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torchvision) (2.6.0+cu118)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torch==2.6.0+cu118->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torch==2.6.0+cu118->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torch==2.6.0+cu118->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torch==2.6.0+cu118->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torch==2.6.0+cu118->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from torch==2.6.0+cu118->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from sympy==1.13.1->torch==2.6.0+cu118->torchvision) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: psutil in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\student\\anaconda3\\envs\\dl\\lib\\site-packages (from jinja2->torch==2.6.0+cu118->torchvision) (2.1.5)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.6/10.0 MB 16.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.0 MB 25.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 19.4 MB/s eta 0:00:00\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: safetensors, regex, tokenizers, accelerate, transformers, evaluate\n",
      "Successfully installed accelerate-1.4.0 evaluate-0.4.3 regex-2024.11.6 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "# 安裝套件\n",
    "!pip install torchvision torchaudio transformers datasets evaluate accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fbabde8-1da5-4118-8264-ce3121e31dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "<torch.cuda.device object at 0x00000223B819B100>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edf31ff7-2e82-49c2-a824-04f8a534607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "import random\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9e0cdaa-5617-4107-997b-642d71fca34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Johnson8187/Chinese_Multi-Emotion_Dialogue_Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c00c3e14-293f-4eae-8bbc-9744c22e9077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       text emotion\n",
      "0                 你要不要去吃午餐？    平淡語氣\n",
      "1                誒誒誒！我甄選上了！    開心語調\n",
      "2        我幾天身體好像有點不太舒服，肚子好痛    悲傷語調\n",
      "3     我的小專題組員都不做事，幹!超後悔跟他一組    憤怒語調\n",
      "4          他們是不是吵架了？不會打起來吧？    平淡語氣\n",
      "...                     ...     ...\n",
      "4154   為什麼每次總是這麼複雜，我到底該怎麼辦？    疑問語調\n",
      "4155      這不應該是這樣，為什麼我總是弄錯？    疑問語調\n",
      "4156       我沒有聽錯吧？這真的是發生了嗎？    疑問語調\n",
      "4157       你到底怎麼想的？這是你的計劃嗎？    疑問語調\n",
      "4158    這真的是我想要的結果嗎？真是太驚訝了！    驚奇語調\n",
      "\n",
      "[4159 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"hf://datasets/Johnson8187/Chinese_Multi-Emotion_Dialogue_Dataset/data.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01d9fe94-e483-4aab-9448-26e8b3a4b580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平淡語氣\n"
     ]
    }
   ],
   "source": [
    "print(df['emotion'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18595c0f-192b-4465-9cd3-30e42fc7d0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\anaconda3\\envs\\dl\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\student\\.cache\\huggingface\\hub\\models--google-bert--bert-base-chinese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 3327/3327 [00:00<00:00, 4293.66 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████| 832/832 [00:00<00:00, 4383.83 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\student\\anaconda3\\envs\\dl\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [156/156 2:47:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.478698</td>\n",
       "      <td>0.853399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('output\\\\tokenizer_config.json',\n",
       " 'output\\\\special_tokens_map.json',\n",
       " 'output\\\\vocab.txt',\n",
       " 'output\\\\added_tokens.json',\n",
       " 'output\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "'''\n",
    "設定 hyperparameters\n",
    "'''\n",
    "model_name = 'google-bert/bert-base-chinese'\n",
    "max_seq_length = 512\n",
    "num_labels = 8  # 修改為8，因為您有8種情緒標籤\n",
    "output_dir = 'output'\n",
    "\n",
    "# 讀取訓練資料\n",
    "df = pd.read_csv(\"hf://datasets/Johnson8187/Chinese_Multi-Emotion_Dialogue_Dataset/data.csv\")\n",
    "sentences = df['text']\n",
    "emotion_map = {\n",
    "    \"平淡語氣\": 0,\n",
    "    \"關切語調\": 1,\n",
    "    \"開心語調\": 2,\n",
    "    \"憤怒語調\": 3,\n",
    "    \"悲傷語調\": 4,\n",
    "    \"疑問語調\": 5,\n",
    "    \"驚奇語調\": 6,\n",
    "    \"厭惡語調\": 7\n",
    "}\n",
    "\n",
    "# 將文字標籤轉換為數字\n",
    "labels = df['emotion'].map(emotion_map)\n",
    "\n",
    "# 載入 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 建立完整的 Dataset（包含文本和標籤）\n",
    "dataset = Dataset.from_dict({\n",
    "    'text': sentences,\n",
    "    'labels': labels\n",
    "})\n",
    "\n",
    "# 切分訓練集和測試集\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# 定義預處理函數\n",
    "def preprocess_function(examples):\n",
    "    # 使用 tokenizer 處理文本\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_seq_length,\n",
    "        return_tensors=None  # 重要：不要在這裡指定 return_tensors\n",
    "    )\n",
    "    # 確保包含標籤\n",
    "    tokenized['labels'] = examples['labels']\n",
    "    return tokenized\n",
    "\n",
    "# 對數據集進行預處理\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names  # 移除原始欄位\n",
    ")\n",
    "\n",
    "# 讀取模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# 設定訓練參數\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=0.00005,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",  # 修正參數名稱\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# 計算模型評估指標\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')  # 使用weighted因為是多分類問題\n",
    "    return {\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "# 設定 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 開始訓練\n",
    "trainer.train()\n",
    "\n",
    "# 儲存模型\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "# 儲存 tokenizer\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d090f53f-d58d-40a4-bba2-7da3389ace2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_2', 'score': 0.8571394681930542},\n",
      " {'label': 'LABEL_4', 'score': 0.9074247479438782},\n",
      " {'label': 'LABEL_4', 'score': 0.9293628931045532},\n",
      " {'label': 'LABEL_2', 'score': 0.6840886473655701},\n",
      " {'label': 'LABEL_0', 'score': 0.9081459045410156}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline,\n",
    ")\n",
    "from pprint import pprint\n",
    "\n",
    "model_dir = './output'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "pipe = pipeline(task='text-classification', model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "texts = [\n",
    "\t\"我每天都能跟她一起上學，我好開心！\",\n",
    "\t\"最好的朋友要離開臺灣了，以後可能不容易再見面...\",\n",
    "\t\"我覺得我快不行了...\",\n",
    "\t\"剛剛收到研究所錄取的通知書！\",\n",
    "\t\"今年的冬天好像比較晚來。\"\n",
    "]\n",
    "result = pipe(texts)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc85f29-479a-4db0-90f8-5d2b3fed2d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from pprint import pprint\n",
    "\n",
    "model_dir = './output'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "pipe = pipeline(task='text-classification', model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# 建立標籤到情緒的反向映射\n",
    "label_to_emotion = {str(v): k for k, v in emotion_map.items()}\n",
    "\n",
    "def predict_with_emotion(texts):\n",
    "    results = pipe(texts)\n",
    "    formatted_results = []\n",
    "\n",
    "    for text, result in zip(texts, results):\n",
    "        # 從 'LABEL_X' 提取數字\n",
    "        label_num = result['label'].split('_')[1]\n",
    "        emotion = label_to_emotion[label_num]\n",
    "        formatted_results.append({\n",
    "            'text': text,\n",
    "            'emotion': emotion,\n",
    "            'confidence': round(result['score'], 3)\n",
    "        })\n",
    "\n",
    "    return formatted_results\n",
    "\n",
    "# 測試文本\n",
    "texts = [\n",
    "\t\"我每天都能跟她一起上學，我好開心！\",\n",
    "\t\"最好的朋友要離開臺灣了，以後可能不容易再見面...\",\n",
    "\t\"我覺得我快不行了...\",\n",
    "\t\"剛剛收到研究所錄取的通知書！\",\n",
    "\t\"今年的冬天好像比較晚來。\"\n",
    "]\n",
    "\n",
    "results = predict_with_emotion(texts)\n",
    "for result in results:\n",
    "    print(f\"{result['text']} => {result['emotion']} ({result['confidence']})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
